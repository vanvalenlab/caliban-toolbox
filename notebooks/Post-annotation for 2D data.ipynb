{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing for Crowd Annotation Pipeline\n",
    "\n",
    "1. Download job report from Figure Eight\n",
    "2. Download annotated images from report\n",
    "3. Clean up annotations (remove small objects, fill small holes, sequentially label the annotations)\n",
    "4. Process annotations and raw images to match each other in size. Can either:\n",
    "    - chop raw images into corresponding pieces\n",
    "    - recombine annotation subimages into full-size images\n",
    "5. Combine raw and annotated images into npz format\n",
    "6. Images are now ready to be used as training data!\n",
    "\n",
    "Files are named by these scripts such that the code blocks can run back-to-back with minimal input. For this reason, it is recommended that users run through the whole pipeline before processing another set of images. The user can specify a few directory names and the \"identifier\" used in pre-annotation and run all cells in the notebook; alternate folder names can be used but this is not recommended.\n",
    "\n",
    "To function properly, your working folder should contain subfolders:\n",
    "- json_logs  \n",
    "    - log from overlapping_chopper ({identifier}\\_overlapping\\_chopper_log.json)\n",
    "- raw images (can be named \"raw\" or something else)\n",
    "\n",
    "The user will also need to supply:\n",
    "- job ID for the data to download from figure eight\n",
    "- API key for figure eight\n",
    "- \"identifier\" to access correct json logs and name files correctly\n",
    "\n",
    "If the default folder names are used, by the end of this pipeline, the working folder (base_dir) will contain the following new files and directories:\n",
    "\n",
    "- CSV/job report downloaded from figure eight\n",
    "- annotations  \n",
    "    - downloaded annotations from figure eight, cleaned, greyscale\n",
    "- (optional) chopped raw images\n",
    "- (optional) stitched annotations\n",
    "- training\n",
    "    - \"all\"\n",
    "        - contains all the raw and annotated images, separated into raw/annotated subfolders but not separated into subfolders beyond that\n",
    "        - other subfolders depending on what \"make_training_data\" needs for subfolder structure\n",
    "- npz (folder)\n",
    "    - .npz file(s) containing training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import stat\n",
    "\n",
    "from deepcell_toolbox.pre_annotation.overlapping_chopper import overlapping_crop_dir\n",
    "from deepcell_toolbox.post_annotation.download_csv import download_and_unzip, save_annotations_from_csv\n",
    "from deepcell_toolbox.post_annotation.clean_montages import clean_montages, relabel_montages, convert_grayscale_all\n",
    "from deepcell_toolbox.post_annotation.montages_to_movies import raw_movie_maker\n",
    "from deepcell_toolbox.post_annotation.overlapping_stitcher import overlapping_stitcher_folder\n",
    "\n",
    "from deepcell_toolbox.utils.data_utils import make_training_data\n",
    "from deepcell_toolbox.utils.io_utils import get_img_names\n",
    "\n",
    "#used to change permissions on folders as they are created\n",
    "#allows user to access folders from file explorer\n",
    "#user can delete intermediate folders (eg, contrast-adjusted raw images) once pipeline is finished\n",
    "#also convenient for moving and editing files (eg, manual correction of images with Fiji)\n",
    "perm_mod = stat.S_IRWXO | stat.S_IRWXU | stat.S_IRWXG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory\n",
    "#base_dir = \"/base/directory/path/here\"\n",
    "#raw_dir = \"/base/directory/path/here/folder_with_fullsize_raw_images\" <- no trailing slash!\n",
    "\n",
    "base_dir = \"/gnv_home/data/testing/post_processing/set2\"\n",
    "raw_dir = \"/gnv_home/data/testing/post_processing/set2/raw\"\n",
    "\n",
    "#identifier given during pre-annotation pipeline; if you're not sure, it's also in the job report csv\n",
    "identifier = \"ISBI_HeLa_fluor_challenge_01\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download job report from Figure Eight\n",
    "By default, this script will download, unzip, and rename the full report from Figure Eight as a .csv file. However, the user can change the report type if one of the other report options is more suitable for their use. (support for other report types not guaranteed with version 0 of this notebook)\n",
    "\n",
    "The user can specify where the zip file should be downloaded and the .csv extracted; by default, the .csv file will be put into a subfolder named CSV (likely the same folder that contained the input data; the CSV files are named to prevent confusion). The report CSV will be renamed \"job_{job_number}\\_{type of report}\\_report.csv\".\n",
    "\n",
    "#### From Figure Eight website:\n",
    "full - Returns the Full report containing every judgment\n",
    "\n",
    "aggregated - Returns the Aggregated report containing the aggregated response for each row\n",
    "\n",
    "json - Returns the JSON report containing the aggregated response, as well as the individual judgments\n",
    "\n",
    "gold_report - Returns the Test Question report\n",
    "\n",
    "workset - Returns the Contributor report\n",
    "\n",
    "source - Returns a CSV of the source data uploaded to the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_to_download = 1380926\n",
    "job_type = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_unzip(job_id_to_download, base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use report to download annotations\n",
    "This script uses the information in the report to download each annotation. Montage annotations will be saved in the \"annotations\" subfolder (it will be created for you by the script).\n",
    "\n",
    "Raw images that could not be annotated (those with \"broken_link = True\") will not be downloaded in this step. If a job contains rows with broken links, the information will be put into two csv files: one, \"job_number_full_report_broken_links.csv\", contains all of the metadata from the full job report, in case the user wants to inspect this for a pattern in the broken links. The other, \"job_number_reupload.csv\", has only the information used to upload the images originally (identifier and annotation_url). If the images are suitable for annotation, the user can easily add this csv to the figure eight job and obtain annotations for those images. (Alternatively, the user may need to go through part of the pre-annotation pipeline to adequately fix and reupload the images in question.)\n",
    "\n",
    "If there are no broken links in the job, or if images with broken links instead of annotations have annotations later on in the job report (if the user has reuploaded those rows to the job), the secondary csv creation will not be triggered.\n",
    "\n",
    "This function returns a list of the image names of any images with broken links; this list will be used later in the pipeline to automatically skip images when stitching images together or making training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(base_dir, \"CSV\")\n",
    "csv_path = os.path.join(csv_dir, \"job_\" + str(job_id_to_download) + \"_\" + job_type + \"_report.csv\")\n",
    "\n",
    "#csv_path = \"/example/path/CSV/job_number_full_report.csv\"\n",
    "\n",
    "annotation_save = os.path.join(base_dir, \"annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_to_drop = save_annotations_from_csv(csv_path, annotation_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean up annotations\n",
    "First, the RGB annotation is converted into grayscale, simplifying downstream use of the annotation.\n",
    "\n",
    "Next, small changes to the morphology of the image are made. Sometimes during annotation, small holes or stray annotations will be submitted, as artifacts of the annotation process. However, these holes or stray pixels don't correspond to what should be annotated, so in this step, we use sci-kit image to fix these small mistakes.\n",
    "\n",
    "Currently uses the old \"clean_montage\" function; this may change in future versions of notebook.\n",
    "\n",
    "After cleaning the annotation, user can optionally run \"relabel_montages\" block, which will relabel the annotations sequentially (eg, perhaps the annotator decided to use the labels 3, 5, and 7 to label cells; this code block would remake the image with labels 1, 2, and 3).\n",
    "\n",
    "The cleaned and relabled annotations will overwrite the downloaded annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_grayscale_all(annotation_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_montages(annotation_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "relabel_montages(annotation_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make raw and annotated images same size\n",
    "Choose to chop up raw images to match the annotations, or to stitch the annotations together to match the original raw images. Recommended that user inspect images for quality of annotations after this step, before moving on to making training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#are the images named with 2D or 3D conventions?\n",
    "is_2D = False\n",
    "num_images = len(get_img_names(raw_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json parameters\n",
    "json_chopper_log_path = os.path.join(base_dir, \"json_logs\", identifier + \"_overlapping_chopper_log.json\")\n",
    "try:\n",
    "    with open(json_chopper_log_path) as json_file:\n",
    "        json_chopper_log = json.load(json_file)\n",
    "except:\n",
    "    print(\"No overlapping_chopper log file found. Is the path to the json file correct?\",\n",
    "          \"\\nIf the images were not chopped prior to annotation, you can skip this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Chop raw images into pieces to match annotation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x_segments = json_chopper_log['num_x_segments']\n",
    "num_y_segments = json_chopper_log['num_y_segments']\n",
    "overlap_perc = json_chopper_log['overlap_perc']\n",
    "frame_offset = json_chopper_log['frame_offset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_crop_dir(raw_dir, \n",
    "                     identifier + \"_raw\", \n",
    "                     num_x_segments, \n",
    "                     num_y_segments, \n",
    "                     overlap_perc, \n",
    "                     frame_offset, \n",
    "                     is_2D)\n",
    "npz_mode = 'chopped'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Recombine annotations to match original raw image size\n",
    "Use this option to combine overlapping annotations into a single image. If a subimage file does not exist, that portion of the larger image will be filled with zeros (the stitched image will still be suitable for training if there were no cells in that part of the image, the usual reason an annotation does not exist).\n",
    "\n",
    "The overlapping stitcher relies on information in the overlapping_chopper json log to function properly. The overlapping stitcher also depends on finding the correct filenames; if the filenames of the image pieces are not what it expects, it will \"stitch\" together empty images. If this is the case, try:\n",
    " - renaming images to match the naming format the stitcher expects\n",
    "\n",
    "OR\n",
    " - create a notebook and copy the overlapping stitcher wrapper into a code block (include imports). Edit the sub_img_format portion of the code to match the format you are passing in. Run the code block to stitch the annotations together. Once the images are stitched you should be able to proceed with this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pieces_dir = '/gnv_home/data/testing/stitcher_new/annotations'\n",
    "pieces_dir = annotation_save\n",
    "#where stitched images should be saved, stitcher will create dir if needed\n",
    "save_dir = os.path.join(base_dir, \"stitched\")\n",
    "#how many full-sized images to be stitched together\n",
    "#num_images = num_images #probably entered earlier in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overlapping_stitcher_folder(pieces_dir, save_dir, identifier, num_images, json_chopper_log, is_2D)\n",
    "npz_mode = 'stitched'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine raw and annotated images into npz format\n",
    "Image files need to be moved into training directories, and possibly subfolders in a particular structure, depending on the training data format of interest. The user can select from several options to make different types of training data (whether changing the images used for training data, or selecting different modes of training data creation). The user is advised to double check that they have selected the appropriate options for their intended use case.\n",
    "\n",
    "If intending to use both chopped and stitched images to make two different npz files, pick one option, go through the end of the notebook to make_training_data, then delete the intermediary \"training\" folder to avoid mixing stitched and chopped images. Then, the user can follow the other option through to the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move raw images and annotations into training folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_dir will hold all subfolders used in make_training_data\n",
    "\n",
    "training_dir = os.path.join(base_dir, \"training\")\n",
    "#training_dir = '/gnv_home/data/testing/stitcher_new/training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose whether to use the chopped raw + annotation directories, or the original raw + stitched annotations\n",
    "\n",
    "if npz_mode == \"stitched\":\n",
    "    raw_img_dir = raw_dir\n",
    "    annotation_dir = save_dir #where overlapping_stitcher saved images\n",
    "    training_raw_folder = \"raw\"\n",
    "    training_annotation_folder = \"stitched\"\n",
    "elif npz_mode == \"chopped\":\n",
    "    raw_img_dir = raw_dir + \"_offset_{0:03d}_chopped_{1:02d}_{2:02d}\".format(frame_offset, num_x_segments, num_y_segments)\n",
    "    annotation_dir = annotation_save\n",
    "    training_raw_folder = \"raw_chopped\"\n",
    "    training_annotation_folder = \"annotations\"\n",
    "    \n",
    "training_raw_dir = os.path.join(training_dir, \"all\", training_raw_folder)\n",
    "training_annotation_dir = os.path.join(training_dir, \"all\", training_annotation_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy raw and annotated images into \"all\" subfolder of training_dir\n",
    "\n",
    "if not os.path.isdir(training_dir):\n",
    "    os.makedirs(training_dir)\n",
    "    os.chmod(training_dir, perm_mod)\n",
    "\n",
    "shutil.copytree(raw_img_dir, training_raw_dir)\n",
    "shutil.copytree(annotation_dir, training_annotation_dir)\n",
    "\n",
    "os.chmod(training_raw_dir, perm_mod)\n",
    "os.chmod(training_annotation_dir, perm_mod)\n",
    "os.chmod(os.path.join(training_dir, 'all'), perm_mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: make subfolders in training folder\n",
    "Use this code block if you intend to make small (manageable) npz movies for tracking/curation. This is not necessary if you are making segmentation training data, or if you want to track a full movie. Make sure to select the appropriate folders in make_training_data if you use this code block.\n",
    "\n",
    "Subfolders are named by the x and y location of the image pieces, and each subfolder contains folders for raw and annotated images. Each subfolder will contain all of the frames of the movie but could be rewritten to allow for parts of movies (eg, 20 sequential frames per folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img_list = get_img_names(training_raw_dir)\n",
    "annotated_img_list = get_img_names(training_annotation_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subfolders for chopped movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_x_segments):\n",
    "    for j in range(num_y_segments):\n",
    "        \n",
    "        #make subfolders\n",
    "        subfolder = \"x_{0:02d}_y_{1:02d}\".format(i,j)\n",
    "        subdir = os.path.join(training_dir, subfolder)\n",
    "        if not os.path.isdir(subdir):\n",
    "            os.makedirs(subdir)\n",
    "            os.chmod(subdir, perm_mod)\n",
    "            \n",
    "        annotation_subdir = os.path.join(subdir, \"annotated\")\n",
    "        if not os.path.isdir(annotation_subdir):\n",
    "            os.makedirs(annotation_subdir)\n",
    "            os.chmod(annotation_subdir, perm_mod)\n",
    "            \n",
    "        raw_subdir = os.path.join(subdir, \"raw\")\n",
    "        if not os.path.isdir(raw_subdir):\n",
    "            os.makedirs(raw_subdir)\n",
    "            os.chmod(raw_subdir, perm_mod)\n",
    "            \n",
    "        #move raw images into subfolders\n",
    "        for raw_img in raw_img_list:\n",
    "            if subfolder in raw_img:\n",
    "                shutil.copy(os.path.join(raw_img_dir, raw_img), raw_subdir)\n",
    "            \n",
    "        #move annotations into subfolders\n",
    "        for annotated_img in annotated_img_list:\n",
    "            if subfolder in annotated_img:\n",
    "                shutil.copy(os.path.join(annotation_dir, annotated_img), annotation_subdir)\n",
    "                \n",
    "training_raw_folder = \"raw\"\n",
    "training_annotation_folder = \"annotated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subfolders to make 2D training data\n",
    "Currently, \"make_training_data\" for 2D data uses only one image per folder. This may be addressed by modifying the way make_training_data loads images into the npz file (future update). Until then, to make 2D training data for segmentations, the user is advised to make subfolders to each contain one raw image and its annotation. The exact structure and content of these subfolders may vary according to use case. The code block below is included as a starting point for the user to move their data appropriately before making 2D training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_images = len(get_img_names(training_raw_dir))\n",
    "\n",
    "for subfolder in range(num_training_images):\n",
    "    subdir = os.path.join(training_dir, \"{0:02d}\".format(subfolder))\n",
    "    if not os.path.isdir(subdir):\n",
    "        os.makedirs(subdir)\n",
    "        os.chmod(subdir, perm_mod)\n",
    "            \n",
    "    annotation_subdir = os.path.join(subdir, \"annotated\")\n",
    "    if not os.path.isdir(annotation_subdir):\n",
    "        os.makedirs(annotation_subdir)\n",
    "        os.chmod(annotation_subdir, perm_mod)\n",
    "            \n",
    "    raw_subdir = os.path.join(subdir, \"raw\")\n",
    "    if not os.path.isdir(raw_subdir):\n",
    "        os.makedirs(raw_subdir)\n",
    "        os.chmod(raw_subdir, perm_mod)\n",
    "        \n",
    "    #move annotations into subfolders\n",
    "    shutil.copy(os.path.join(training_annotation_dir, annotated_img_list[subfolder]), annotation_subdir)\n",
    "    \n",
    "    #move raw images into subfolders\n",
    "    shutil.copy(os.path.join(training_raw_dir, raw_img_list[subfolder]), raw_subdir)\n",
    "\n",
    "training_raw_folder = \"raw\"\n",
    "training_annotation_folder = \"annotated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make training data\n",
    "Make sure to choose the appropriate border and output mode for what you intend to do! Movies intended to be tracked should use border_mode = 'same' and output_mode = 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#border_mode = 'valid'\n",
    "border_mode = 'same'\n",
    "\n",
    "#output_mode = 'disc'\n",
    "#output_mode = 'sample'\n",
    "output_mode = 'conv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(base_dir, 'npz')\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    os.chmod(output_dir, perm_mod)\n",
    "\n",
    "npz_name = \"{0}_{1}_{2}_{3}.npz\".format(identifier, npz_mode, border_mode, output_mode)\n",
    "file_name_save = os.path.join(output_dir, npz_name)\n",
    "\n",
    "#Training directories are organized according to location within an image\n",
    "#if there are any movies that shouldn't be included in the npz\n",
    "#(unsuitable for training, or don't need to be tracked), put them in \"samples_to_drop\"\n",
    "#\"samples_to_drop\" does not yet automatically update \n",
    "#based on \"images_to_drop\" (from downloading annotations), but will in the future\n",
    "\n",
    "if npz_mode == \"stitched\" and is_2D == False:\n",
    "    training_direcs = [\"all\"]\n",
    "elif npz_mode == \"chopped\" and is_2D == False:\n",
    "    training_direcs = ['x_0{}_y_0{}'.format(i,j) for i in range(num_x_segments) for j in range(num_y_segments)]\n",
    "    samples_to_drop = [\"all\"]\n",
    "    training_direcs = [x for x in training_direcs if x not in samples_to_drop]\n",
    "elif is_2D == True:\n",
    "    training_direcs = ['{0:02d}'.format(subfolder) for subfolder in range(num_training_images)]\n",
    "    samples_to_drop = ['all']\n",
    "    training_direcs = [x for x in training_direcs if x not in samples_to_drop]\n",
    "\n",
    "channel_names = [\"\"] # Commonality in raw filenames\n",
    "\n",
    "if is_2D:\n",
    "    dimensionality = 2\n",
    "    kwargs = {'max_training_examples': 1e7,\n",
    "             'dilation_radius': 1,\n",
    "             'distance_bins': 4,\n",
    "             'distance_transform': False}\n",
    "else:\n",
    "    dimensionality = 3\n",
    "    kwargs = {\"num_frames\": num_images,\n",
    "             \"annotation_name\": [\"\"],\n",
    "             \"montage_mode\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training data\n",
    "make_training_data(training_dir, \n",
    "                    file_name_save, \n",
    "                    channel_names, \n",
    "                    dimensionality,\n",
    "                    training_direcs,\n",
    "                    window_size_x=30,\n",
    "                    window_size_y=30,\n",
    "                    edge_feature=[1, 0, 0],\n",
    "                    border_mode=border_mode,\n",
    "                    output_mode=output_mode,\n",
    "                    raw_image_direc= training_raw_folder,\n",
    "                    annotation_direc= training_annotation_folder,\n",
    "                    verbose=False,\n",
    "                    reshape_size=None,\n",
    "                    **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the result\n",
    "data = np.load(file_name_save)\n",
    "X_to_load, y_to_load = data['X'][()], data['y'][()]\n",
    "\n",
    "print(data.keys())\n",
    "data_readable_X, data_readable_y = data['X'][()], data['y'][()]\n",
    "print('X Shape:', data_readable_X.shape)\n",
    "print('y Shape:', data_readable_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
