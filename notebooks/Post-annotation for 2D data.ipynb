{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing for Crowd Annotation Pipeline\n",
    "\n",
    "1. Download job report from Figure Eight\n",
    "2. Download annotated images from report\n",
    "3. Clean up annotations (remove small objects, fill small holes, sequentially label the annotations)\n",
    "4. Process annotations and raw images to match each other in size. Can either:\n",
    "    - chop raw images into corresponding pieces\n",
    "    - recombine annotation subimages into full-size images\n",
    "5. Combine raw and annotated images into npz format\n",
    "6. Images are now ready to be used as training data!\n",
    "\n",
    "Files are named by these scripts such that the code blocks can run back-to-back with minimal input. For this reason, it is recommended that users run through the whole pipeline before processing another set of images. The user can specify a few directory names and the \"identifier\" used in pre-annotation and run all cells in the notebook; alternate folder names can be used but this is not recommended.\n",
    "\n",
    "To function properly, your working folder should contain subfolders:\n",
    "- json_logs  \n",
    "    - log from overlapping_chopper ({identifier}\\_overlapping\\_chopper_log.json)\n",
    "- raw images (can be named \"raw\" or something else)\n",
    "\n",
    "The user will also need to supply:\n",
    "- job ID for the data to download from figure eight\n",
    "- API key for figure eight\n",
    "- \"identifier\" to access correct json logs and name files correctly\n",
    "\n",
    "If the default folder names are used, by the end of this pipeline, the working folder (base_dir) will contain the following new files and directories:\n",
    "\n",
    "- CSV/job report downloaded from figure eight\n",
    "- annotations  \n",
    "    - downloaded montages from figure eight, cleaned\n",
    "- (optional) chopped raw images\n",
    "- .npz file containing training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import os\n",
    "\n",
    "from deepcell_toolbox.post_annotation.download_csv import download_and_unzip, save_annotations_from_csv\n",
    "from deepcell_toolbox.post_annotation.clean_montages import clean_montages, relabel_montages, convert_grayscale_all\n",
    "from deepcell_toolbox.post_annotation.montages_to_movies import raw_movie_maker\n",
    "#from deepcell_toolbox.post_annotation.overlapping_stitcher import overlapping_stitcher\n",
    "\n",
    "from deepcell_toolbox.utils.data_utils import make_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory\n",
    "#base_dir = \"/base/directory/path/here\"\n",
    "#raw_dir = \"/base/directory/path/here/folder_with_fullsize_raw_images\"\n",
    "\n",
    "base_dir = \"/gnv_home/data/2D_upload_test/set0\"\n",
    "raw_dir = \"/gnv_home/data/2D_upload_test/set0/raw\"\n",
    "\n",
    "#identifier given during pre-annotation pipeline; if you're not sure, it's also in the job report csv\n",
    "identifier = \"ecoli_s0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download job report from Figure Eight\n",
    "By default, this script will download, unzip, and rename the full report from Figure Eight as a .csv file. However, the user can change the report type if one of the other report options is more suitable for their use. (support for other report types not guaranteed with version 0 of this notebook)\n",
    "\n",
    "The user can specify where the zip file should be downloaded and the .csv extracted; by default, the .csv file will be put into a subfolder named CSV (likely the same folder that contained the input data; the CSV files are named to prevent confusion). The report CSV will be renamed \"job_{job_number}\\_{type of report}\\_report.csv\".\n",
    "\n",
    "#### From Figure Eight website:\n",
    "full - Returns the Full report containing every judgment\n",
    "\n",
    "aggregated - Returns the Aggregated report containing the aggregated response for each row\n",
    "\n",
    "json - Returns the JSON report containing the aggregated response, as well as the individual judgments\n",
    "\n",
    "gold_report - Returns the Test Question report\n",
    "\n",
    "workset - Returns the Contributor report\n",
    "\n",
    "source - Returns a CSV of the source data uploaded to the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_to_download = 1366009\n",
    "job_type = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_unzip(job_id_to_download, base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use report to download annotations\n",
    "This script uses the information in the report to download each annotation. Montage annotations will be saved in the \"annotations\" subfolder (it will be created for you by the script).\n",
    "\n",
    "Raw images that could not be annotated (those with \"broken_link = True\") will not be downloaded in this step. If a job contains rows with broken links, the information will be put into two csv files: one, \"job_number_full_report_broken_links.csv\", contains all of the metadata from the full job report, in case the user wants to inspect this for a pattern in the broken links. The other, \"job_number_reupload.csv\", has only the information used to upload the images originally (identifier and annotation_url). If the images are suitable for annotation, the user can easily add this csv to the figure eight job and obtain annotations for those images. (Alternatively, the user may need to go through part of the pre-annotation pipeline to adequately fix and reupload the images in question.)\n",
    "\n",
    "If there are no broken links in the job, or if images with broken links instead of annotations have annotations later on in the job report (if the user has reuploaded those rows to the job), the secondary csv creation will not be triggered.\n",
    "\n",
    "This function returns a list of the image names of any images with broken links; this list will be used later in the pipeline to automatically skip images when stitching images together or making training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(base_dir, \"CSV\")\n",
    "csv_path = os.path.join(csv_dir, \"job_\" + str(job_id_to_download) + \"_\" + job_type + \"_report.csv\")\n",
    "\n",
    "#csv_path = \"/example/path/CSV/job_number_full_report.csv\"\n",
    "\n",
    "annotation_save = os.path.join(base_dir, \"annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images_to_drop = save_annotations_from_csv(csv_path, annotation_save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean up annotations\n",
    "First, the RGB annotation is converted into grayscale, simplifying downstream use of the annotation.\n",
    "\n",
    "Next, small changes to the morphology of the image are made. Sometimes during annotation, small holes or stray annotations will be submitted, as artifacts of the annotation process. However, these holes or stray pixels don't correspond to what should be annotated, so in this step, we use sci-kit image to fix these small mistakes.\n",
    "\n",
    "Currently uses the old \"clean_montage\" function; this may change in future versions of notebook.\n",
    "\n",
    "After cleaning the annotation, user can optionally run \"relabel_montages\" block, which will relabel the annotations sequentially (eg, perhaps the annotator decided to use the labels 3, 5, and 7 to label cells; this code block would remake the image with labels 1, 2, and 3).\n",
    "\n",
    "The cleaned and relabled annotations will overwrite the downloaded annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotations_folder = annotation_save\n",
    "#annotations_folder = \"/base/directory/path/here/wherever_you_moved_the_annotations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_grayscale_all(annotations_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_montages(annotations_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "relabel_montages(annotations_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make raw and annotated images same size\n",
    "Depending on which option you choose (for 2D, option 2 is recommended), the file organization may change\n",
    "In each option:\n",
    " - code block 1: read/set parameters\n",
    " - code block 2: modify images\n",
    "    - chopper should chop all\n",
    "    - move anything not in images_to_drop into training folder\n",
    "    - stitcher should skip stitching anything that contains an image_to_drop\n",
    "    - stitcher should move files into training folder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Chop raw images into pieces to match annotation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move overlapping_chopper base function into utils\n",
    "# read json parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the chop parameters to populate training_direcs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Recombine annotations to match original raw image size\n",
    "- basic functionality to recombine annotations\n",
    "- tries to combine labels for annotations that overlap to reduce cells that are annotated half as one thing and half as another\n",
    "- does not support images_to_drop yet\n",
    "- may encounter problems if original image size is not divisible by num_segments\n",
    "    - this would be a problem for npz formation; image stitching would work fine, but the image sizes may be off\n",
    "- overlapping_stitcher_2D wrapper definition will live here for now until more features are added, then it will be moved into overlapping_stitcher.py    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "import sys\n",
    "\n",
    "def overlapping_stitcher_2D(pieces_dir, num_images, save_dir, num_x, num_y, overlap_perc, identifier):\n",
    "    '''\n",
    "    wrapper function for using the overlapping stitcher on 2D annotations from figure eight\n",
    "    '''\n",
    "        \n",
    "    #directories\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        #add folder modification permissions to deal with files from file explorer\n",
    "        mode = stat.S_IRWXO | stat.S_IRWXU | stat.S_IRWXG\n",
    "        os.chmod(save_dir, mode)\n",
    "    \n",
    "    for image in range(num_images):\n",
    "        sub_img_format = \"_img_{0:02d}\".format(image)\n",
    "\n",
    "        save_name = identifier + sub_img_format + \"_annotation.png\"       \n",
    "        \n",
    "        sub_img_format = \"{0}\" + sub_img_format + \"_x_{1:02d}_y_{2:02d}_annotation.png\" \n",
    "        \n",
    "        overlapping_stitcher(pieces_dir, save_dir, num_x, num_y, overlap_perc, identifier, sub_img_format, save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Combine raw and annotated images into npz format\n",
    "- need to figure how to structure training_direcs\n",
    "- not really supported yet in this (2D) notebook, this is more of a placeholder code block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import stat\n",
    "import sys\n",
    "import pathlib\n",
    "\n",
    "#get info from json logs\n",
    "#log_folder = os.path.join(base_dir, \"json_logs\")\n",
    "#chop_params = read_json_params_montage(log_folder, identifier)\n",
    "#number of frames per montage\n",
    "#num_frames = montage_params[0]\n",
    "#segments the original image was chopped into:\n",
    "#num_x = montage_params[1]\n",
    "#num_y = montage_params[2]\n",
    "\n",
    "# Load data\n",
    "base_dir = '/gnv_home/data/overlapping_stitcher_test'\n",
    "output_directory = os.path.join(base_dir, \"npz\")\n",
    "#output_directory = '/gnv_home/data/Ed/3T3/set0'\n",
    "file_name_save = os.path.join(output_directory, 'ecoli_s0_retry_same.npz')\n",
    "\n",
    "#Training directories are organized according to location within an image\n",
    "#if there are any movies that shouldn't be included in the npz\n",
    "#(unsuitable for training, or don't need to be tracked), put them in \"samples_to_drop\"\n",
    "samples_to_drop = []\n",
    "training_direcs = [\"training\"]\n",
    "#training_direcs = ['x_0{}_y_0{}'.format(i,j) for i in range(num_x) for j in range(num_y)]\n",
    "#training_direcs = [x for x in training_direcs if x not in samples_to_drop]\n",
    "channel_names = [\"\"] # Commonality in raw filenames\n",
    "\n",
    "# Create output directory, if necessary\n",
    "pathlib.Path(output_directory).mkdir(parents=True, exist_ok=True)\n",
    "#mode = stat.S_IRWXO | stat.S_IRWXU | stat.S_IRWXG\n",
    "#os.chmod(output_directory, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 3\n",
      "Number of training data points: 2585664\n",
      "Class weights: [1.]\n"
     ]
    }
   ],
   "source": [
    "# Create the training data\n",
    "make_training_data(\n",
    "    direc_name = base_dir,\n",
    "    file_name_save = file_name_save,\n",
    "    channel_names = channel_names,\n",
    "    dimensionality = 2,\n",
    "    training_direcs = training_direcs,\n",
    "    raw_image_direc = \"raw\",\n",
    "    annotation_direc = \"stitched_annotations\",\n",
    "    annotation_name = \"\",\n",
    "    border_mode = \"same\",\n",
    "    output_mode = \"conv\",\n",
    "    reshape_size = None,\n",
    "    verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
