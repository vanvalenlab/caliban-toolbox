{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caliban Fig8 Upload Pipeline\n",
    "This pipeline creates a Figure Eight job and uploads data files to an S3 bucket for data curation.\n",
    "\n",
    "- Resize npz into smaller pieces (image dimensions) if needed\n",
    "- Shorten length of npz if needed\n",
    "- Upload files to AWS\n",
    "- Create job csv and upload it to figure 8\n",
    "\n",
    "**Note: if you need to start a Caliban job to correct the results of a previous Caliban job, please set \"base_dir\" then skip to the end of the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from __future__ import absolute_import\n",
    "\n",
    "from segmentation.utils import data_utils\n",
    "from imageio import imread, volread, imwrite, volwrite\n",
    "import numpy as np\n",
    "import os\n",
    "import stat\n",
    "import sys\n",
    "\n",
    "\n",
    "import skimage.io as io\n",
    "\n",
    "from caliban_toolbox.pre_annotation import npz_preprocessing\n",
    "from caliban_toolbox.post_annotation import npz_postprocessing\n",
    "from caliban_toolbox.pre_annotation.aws_upload import aws_caliban_upload\n",
    "from caliban_toolbox.pre_annotation.caliban_csv import initial_csv_maker, create_next_CSV\n",
    "from caliban_toolbox.pre_annotation.fig_eight_upload import fig_eight\n",
    "\n",
    "\n",
    "\n",
    "from ipywidgets import fixed, interactive\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import filters, img_as_uint\n",
    "import skimage as sk\n",
    "import xarray as xr\n",
    "\n",
    "from caliban_toolbox.utils.io_utils import get_img_names\n",
    "from caliban_toolbox.utils import widget_utils\n",
    "\n",
    "perm_mod = stat.S_IRWXO | stat.S_IRWXU | stat.S_IRWXG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for model training\n",
    "We'll specify which channels will be used to generate preliminary labels for the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3D data:\n",
    "data_dir = '/example_data/3D/FOV_0'\n",
    "data_vol = os.path.join(data_dir, 'Pos0_DAPIRegistered.tif')\n",
    "data_stack_values = volread(data_vol)\n",
    "data_stack_values = np.expand_dims(data_stack, axis=-1)\n",
    "\n",
    "# convert to xarray\n",
    "fov_names = [\"slice_\" + str(x) for x in range(data_stack.shape[0])]\n",
    "channel_names = [\"DAPI\"]\n",
    "data_stack_xr = xr.DataArray(data_stack_values, coords=[fov_names, range(data_stack.shape[1]),\n",
    "                                                range(data_stack.shape[2]), channel_names],\n",
    "                            dims=[\"fovs\", \"rows\", \"cols\", \"channels\"])\n",
    "\n",
    "data_stack_xr.coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series data\n",
    "data_stack_xr = data_utils.load_tifs_from_points_dir(\"/example_data/timelapse/HeLa_by_image\", \n",
    "                                                  points=os.listdir(\"/example_data/timelapse/HeLa_by_image\"),\n",
    "                                                 tifs=[\"FITC_001.png\", \"Phase_000.png\", \"Phase_001.png\", \"Phase_002.png\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the data through the network to produce labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepcell upload code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess the deepcell labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to postprocess labels, select appropriate parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Adjust image contrast, background, thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to hold newly created channels\n",
    "adjusted_channels, adjusted_channel_names, adjusted_channel_kwargs = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list1 = [\"hi\", \"hello\", \"me\", \"not me\"]\n",
    "np.where(data_stack_xr.channels.values == \"Phase_001\")[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_img_from_stack(stack, slice_idx, chan_name):\n",
    "    \"\"\"Helper function for interatively selecting an image to test out modifications from a stack of images\n",
    "\n",
    "    Inputs\n",
    "        stack: a 4D stack of images [slices, rows, cols, channels]\n",
    "        slice_index: index of sliced image to use\n",
    "        chan_index: index of the channel to use\n",
    "\n",
    "    Returns\n",
    "        slice_index: the slice index that was selected by the user\n",
    "        chan_index: the chan_index that was selected by the user\"\"\"\n",
    "    chan_idx = np.where(stack.channels.values == chan_name)[0][0]\n",
    "    img = stack[slice_idx, :, :, chan_idx]\n",
    "    fig, ax = plt.subplots(figsize=(16, 12))\n",
    "    ax.imshow(img, cmap=mpl.cm.gray)\n",
    "    return slice_idx, chan_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Pick raw image\n",
    "This will be used as an example to display effect of adjustments later on. This can be changed at any time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b0604aa3984831bf4f1715b82fb769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='slice_idx', max=4), Dropdown(description='chan_name', op…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "choose_img_output = interactive(choose_img_from_stack, stack = fixed(data_stack_xr), \n",
    "                         slice_idx = (0, data_stack.shape[0]-1, 1),\n",
    "                         chan_name = (data_stack_xr.channels.values));\n",
    "choose_img_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Set raw image adjust parameters\n",
    "These will be held in memory until npz is saved. A record of the contrast adjustment settings will be saved with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94bc2fcafb704614b39b2a717cfd40d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='blur', max=4.0), Checkbox(value=True, description='s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get most recent parameters for selected image\n",
    "selected_slice_idx, selected_channel_idx = choose_img_output.result\n",
    "img = data_stack_xr[selected_slice_idx, :, :, selected_channel_idx]\n",
    "\n",
    "# interative edit mode\n",
    "adjust_image_output = interactive(widget_utils.adjust_image_interactive, image=fixed(img), blur=(0.0,4,0.1), \n",
    "                       gamma_adjust=(0.1,4,0.1), sobel_factor=(10,10000,100), v_min = (0, 255, 1), \n",
    "                       v_max = (0, 255, 1));\n",
    "adjust_image_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Adjust raw image with specified parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create placeholder channel to hold the output of channel adjustment\n",
    "adjusted_channel_xr = xr.DataArray(np.zeros(data_stack_xr.shape[:-1] + (1,), np.uint8),\n",
    "                                   coords=[data_stack_xr.points, data_stack_xr.rows, data_stack_xr.cols,\n",
    "                                          [\"adjusted_channel\"]],\n",
    "                                   dims=data_stack_xr.dims)\n",
    "\n",
    "# adjust all slices for given channel\n",
    "for i in range(data_stack_xr.shape[0]):\n",
    "    image = data_stack_xr[i, :, :, selected_channel_idx]\n",
    "    adjusted_channel_xr[i, :, :, 0] = widget_utils.adjust_image(image, adjust_image_output.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verify that adjustment looks good across slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdc1b92776344b29fa218da8ad0d0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='slice_idx', max=4), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_adjustment_output = interactive(choose_img_from_stack, stack = fixed(adjusted_channel_xr), \n",
    "                               slice_idx = (0, data_stack.shape[0]-1, 1),\n",
    "                              chan_name = fixed(\"adjusted_channel\"));\n",
    "check_adjustment_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Give the adjusted channel an informative name, repeat part A for each channel that needs to be adjusted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjusted_channel_name = [\"phase_contrast_adjusted\"]\n",
    "adjusted_channel_names.append(adjusted_channel_name)\n",
    "adjusted_channels.append(adjusted_channel_xr.values)\n",
    "adjusted_channel_kwargs.append(adjust_image_output.kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: create overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize lists to hold newly created channels\n",
    "combined_channels, combined_channel_names, combined_channel_kwargs = [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Select first channel to be included in overlay, perform any needed adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FITC_001', 'Phase_000', 'Phase_001', 'Phase_002'], dtype='<U9')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# available channels\n",
    "data_stack_xr.channels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de23a77c52b04b87b56a8e3213ebea61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='blur', max=4.0), Checkbox(value=True, description='s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_1_idx = 0\n",
    "img1 = data_stack[selected_slice_idx, :, :, img_1_idx]\n",
    "\n",
    "adjust_overlay_1_output = interactive(widget_utils.adjust_image_interactive, image=fixed(img1), blur=(0.0,4,0.1), \n",
    "                          gamma_adjust=(0.1,4,0.1), sobel_factor=(10,10000,100), v_min = (0, 255, 1), \n",
    "                          v_max = (0, 255, 1));\n",
    "adjust_overlay_1_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Select second channel to be included in overlay, perform any needed adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab0c1c0e85e44e0fa518ed849271f97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='blur', max=4.0), Checkbox(value=True, description='s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_2_idx = 3\n",
    "img2 = data_stack[selected_slice_idx, :, :, img_2_idx]\n",
    "\n",
    "adjust_overlay_2_output = interactive(widget_utils.adjust_image_interactive, image=fixed(img2), blur=(0.0,4,0.1), \n",
    "                          gamma_adjust=(0.1,4,0.1), sobel_factor=(2,10000,100), v_min = (0, 255, 1), \n",
    "                          v_max = (0, 255, 1));\n",
    "adjust_overlay_2_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Select the appropriate settings for combinging the two images together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bad8f978f42488aaa655ce127851e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='prop_img_1', max=1.0), IntSlider(value=0, descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "overlay_images_output = interactive(widget_utils.overlay_images_interactive, \n",
    "                               img_1 = fixed(adjust_overlay_1_output.result), \n",
    "                               img_2 = fixed(adjust_overlay_2_output.result), \n",
    "                               prop_img_1 =(0,1.0, 0.1), v_min = (0, 255, 1), v_max = (0, 255, 1))\n",
    "overlay_images_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create overlays across the whole image stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_settings = overlay_images_output.kwargs\n",
    "prop_img_1 = combined_settings['prop_img_1']\n",
    "v_min = combined_settings['v_min']\n",
    "v_max = combined_settings['v_max']\n",
    "\n",
    "overlay_channel_xr = xr.DataArray(np.zeros(data_stack_xr.shape[:-1] + (1,), np.uint8),\n",
    "                                   coords=[data_stack_xr.points, data_stack_xr.rows, data_stack_xr.cols,\n",
    "                                          [\"overlay_channel\"]],\n",
    "                                   dims=data_stack_xr.dims)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(data_stack.shape[0]):\n",
    "    image1 = data_stack[i, :, :, img_1_idx]\n",
    "    image2 = data_stack[i, :, :, img_2_idx]\n",
    "\n",
    "    image1_adjusted = widget_utils.adjust_image(image1, adjust_overlay_1_output.kwargs)\n",
    "    image2_adjusted = widget_utils.adjust_image(image2, adjust_overlay_2_output.kwargs)\n",
    "    overlay_channel_xr[i, :, :, 0] = widget_utils.overlay_images(image1_adjusted, image1_adjusted, \n",
    "                                                              prop_img_1, v_min, v_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cc19e0edf340aaab96ee84c2bc55bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='slice_idx', max=4), Output()), _dom_classes=('widget-int…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_adjustment = interactive(choose_img_from_stack, stack = fixed(overlay_channel_xr), \n",
    "                         slice_idx = (0, data_stack.shape[0]-1, 1),\n",
    "                         chan_name = fixed(\"overlay_channel\"));\n",
    "check_adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Give the combined channel an informative name, repeat part B for each set of overlays that needs to be constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give this overlay an informative name\n",
    "combined_channel_name = [\"phase_contrast_overlay\"]\n",
    "\n",
    "# add metadata to list\n",
    "combined_channel_names.append(adjusted_channel_name)\n",
    "combined_channels.append(overlay_channel_xr.values)\n",
    "adjusted_channel_kwargs.append([prop_img_1, v_min, v_max, adjust_overlay_1_output.kwargs, adjust_overlay_2_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Determine which channels will be included and in what order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapt MIBI channel reordering code here to work for above data as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Set shape of NPZ files for optimum annotator ease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split npz into pieces\n",
    "The idea is that you'd be starting from one huge npz and breaking it into managable pieces. This part will probably include:\n",
    "- reshape npz (size of each piece is smaller, but same number of frames)\n",
    "- break up each npz into fewer frames (annotator does not necessarily need to do all 30+ frames of a movie at once)\n",
    "- save these reshaped pieces as individual npz files, so they can be uploaded and worked on separately\n",
    "- relabel the npzs as needed (choose between no relabel, relabel each cell in each frame to have unique label, or relabel to have unique labels but preserve 3D relationships)\n",
    "\n",
    "These pieces will need specific names so that we can put them back together again if needed (especially putting frames back into longer contiguous movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../example_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape npz (y and x dimensions) if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_npz_path = '/data/figure_eight/HeLa-S3_cyto_movies/set7/HeLa_movie_s7_uncorrected_fullsize_all_channels.npz'\n",
    "x_size = 320\n",
    "y_size = 270\n",
    "reshaped_save_dir = os.path.join(base_dir, 'reshaped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_npz(full_npz_path, x_size, y_size, save_dir = reshaped_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop npz into smaller x and y pieces\n",
    "optionally takes an npz file and splits it into many smaller npzs, each of which can be submitted a separate Figure8 job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice (t or z dimension) single npz if needed\n",
    "#### Use this option if you do not need to reshape y and x in your npz before beginning work.\n",
    "\n",
    "This option may be useful in the future for something like curation of 2D npzs. In that case, each slice is independent, so only one slice would be launched at a time. With only one frame to annotate, the annotation time per image allows for a larger image (annotating a larger image would also reduce the number of annotations on the edge, which can be tougher since there is less context to judge cell boundaries).\n",
    "\n",
    "However, this option has limited use until Caliban supports zooming in and out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_npz_path = ''\n",
    "# batch_size = 1\n",
    "# sliced_save_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_npz_batches(full_npz_path, batch_size, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel npzs -- recommended for fig8 first pass jobs\n",
    "\"Predict\" relabeling is recommended (unless 3D segmentation models are being used), since this relabeling strategy will perform decently on most 3D data to reduce the human labor involved in correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabel_npzs_folder(npz_dir = sliced_save_dir, relabel_type = 'predict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
