{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caliban Fig8 Upload Pipeline\n",
    "This pipeline creates a Figure Eight job and uploads data files to an S3 bucket for data curation.\n",
    "\n",
    "- Resize npz into smaller pieces (image dimensions) if needed\n",
    "- Shorten length of npz if needed\n",
    "- Upload files to AWS\n",
    "- Create job csv and upload it to figure 8\n",
    "\n",
    "**Note: if you need to start a Caliban job to correct the results of a previous Caliban job, please set \"base_dir\" then skip to the end of the notebook.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "from __future__ import absolute_import\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import skimage.io as io\n",
    "\n",
    "from deepcell_toolbox.pre_annotation.npz_preprocessing import reshape_npz, slice_npz_batches, slice_npz_folder, relabel_npzs_folder, crop_npz\n",
    "from deepcell_toolbox.post_annotation.npz_postprocessing import reconstruct_npz\n",
    "from deepcell_toolbox.pre_annotation.aws_upload import aws_caliban_upload\n",
    "from deepcell_toolbox.pre_annotation.caliban_csv import initial_csv_maker, create_next_CSV\n",
    "from deepcell_toolbox.pre_annotation.fig_eight_upload import fig_eight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data for model training\n",
    "We'll specify which channels will be used to generate preliminary labels from the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the data through the network to produce labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deepcell upload code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocess the deepcell labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to postprocess labels, select appropriate parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create combined image stack\n",
    "\n",
    "combine labels with imaging channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../example_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the segmentation masks and channel TIFs are in different xarrays, we'll combine them together\n",
    "channel_xr = xr.open_dataarray(os.path.join(base_dir, \"segmentation_channels.xr\"))\n",
    "labels_xr = xr.open_dataarray(os.path.join(base_dir, \"segmentation_labels.xr\"))\n",
    "\n",
    "combined_xr = data_utils.combine_xarrays((channel_xr, labels_xr), axis=-1)\n",
    "combined_xr.to_netcdf(os.path.join(base_dir, \"combined_xr.xr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split npz into pieces\n",
    "The idea is that you'd be starting from one huge npz and breaking it into managable pieces. This part will probably include:\n",
    "- reshape npz (size of each piece is smaller, but same number of frames)\n",
    "- break up each npz into fewer frames (annotator does not necessarily need to do all 30+ frames of a movie at once)\n",
    "- save these reshaped pieces as individual npz files, so they can be uploaded and worked on separately\n",
    "- relabel the npzs as needed (choose between no relabel, relabel each cell in each frame to have unique label, or relabel to have unique labels but preserve 3D relationships)\n",
    "\n",
    "These pieces will need specific names so that we can put them back together again if needed (especially putting frames back into longer contiguous movies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../example_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape npz (y and x dimensions) if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_npz_path = '/data/figure_eight/HeLa-S3_cyto_movies/set7/HeLa_movie_s7_uncorrected_fullsize_all_channels.npz'\n",
    "x_size = 320\n",
    "y_size = 270\n",
    "reshaped_save_dir = os.path.join(base_dir, 'reshaped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape_npz(full_npz_path, x_size, y_size, save_dir = reshaped_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop npz into smaller x and y pieces\n",
    "optionally takes an npz file and splits it into many smaller npzs, each of which can be submitted a separate Figure8 job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice (t or z dimension) single npz if needed\n",
    "#### Use this option if you do not need to reshape y and x in your npz before beginning work.\n",
    "This option may be useful in the future for something like curation of 2D npzs. In that case, each slice is independent, so only one slice would be launched at a time. With only one frame to annotate, the annotation time per image allows for a larger image (annotating a larger image would also reduce the number of annotations on the edge, which can be tougher since there is less context to judge cell boundaries).\n",
    "\n",
    "However, this option has limited use until Caliban supports zooming in and out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_npz_path = ''\n",
    "# batch_size = 1\n",
    "# sliced_save_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice_npz_batches(full_npz_path, batch_size, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slice (t or z dimension) folder of reshaped npzs if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "sliced_save_dir = os.path.join(base_dir, 'reshaped_resized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slice_npz_folder(src_folder = reshaped_save_dir, batch_size = batch_size, save_dir = sliced_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relabel npzs -- recommended for fig8 first pass jobs\n",
    "\"Predict\" relabeling is recommended (unless 3D segmentation models are being used), since this relabeling strategy will perform decently on most 3D data to reduce the human labor involved in correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relabel_npzs_folder(npz_dir = sliced_save_dir, relabel_type = 'predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload pieces to AWS\n",
    "\n",
    "### Select directory\n",
    "\n",
    "\"base_dir\" is a directory that holds the desired .npz or .trk file folder for data curation. This will go through the folder and upload the files to the desired S3 input bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dir = sliced_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bucket to load uncurated files\n",
    "input_bucket = \"caliban-input\"\n",
    "\n",
    "# bucket for curated files after submission\n",
    "output_bucket = \"caliban-output\"\n",
    "\n",
    "# subfolders in input/output bucket\n",
    "aws_folder = \"HeLa-S3/cyto/Stanford_movies/set7\"\n",
    "\n",
    "# stage becomes another subfolder so that data from subsequent jobs are grouped nearby\n",
    "stage = 'firstpass'\n",
    "\n",
    "filenames, filepaths = aws_caliban_upload(input_bucket, \n",
    "                                          output_bucket, \n",
    "                                          aws_folder, \n",
    "                                          stage, \n",
    "                                          folder_to_upload = upload_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create CSV file for Figure Eight\n",
    "\n",
    "\n",
    "Figure Eight jobs can be created easily by using a CSV file where each row contains information about one task. To create jobs for caliban, each row has a unique url that directs users to the Caliban tool with the correct data to curate. The CSV file is saved as {identifier}\\_{stage}\\_upload.csv in a folder that only holds CSVs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(base_dir, \"CSV\")\n",
    "identifier = \"HeLa-S3_cyto_movies_s7\"\n",
    "\n",
    "initial_csv_maker(csv_dir = csv_dir, \n",
    "                  identifier = identifier,\n",
    "                  stage = stage,\n",
    "                  input_bucket = input_bucket,\n",
    "                  output_bucket = output_bucket,\n",
    "                  subfolders = aws_folder,\n",
    "                  filenames = filenames,\n",
    "                  filepaths = filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Figure Eight job\n",
    "\n",
    "The Figure Eight API allows us to create a new job and upload data to it from this notebook. However, since our jobs don't include required test questions, editing job information such as the title of the job must be done via the website. This section of the notebook uses the API to create a job and upload data to it, then reminds the user to finish editing the job on the website.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_to_copy =  1463619 #Caliban first-pass job\n",
    "# job_id_to_copy = _ #Caliban foreground/background correction job\n",
    "# job_id_to_copy = _ #Caliban inter-cell fixes job\n",
    "# job_id_to_copy = _ #Caliban tracking/lineage correction job\n",
    "fig_eight(csv_dir, \"{0}_{1}\".format(identifier, stage), job_id_to_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Figure Eight job on results of previous job\n",
    "Due to the complexity of Caliban jobs, full annotation correction takes place over several jobs, which each focus on correcting a different aspect of the file. To begin the next job in a sequence, the files must be moved from one bucket location (their output location) to the input location for the next job. A new CSV file must also be created for the next job.\n",
    "\n",
    "In this step, the job report from the finished job is downloaded to the CSV folder. The user must input the \"next_stage\" that the next Figure 8 job will be focusing on; this information will supplement the information in the job report so that the appropriate files can be moved and the CSV file created. After this step has finished, use the \"Create Figure Eight job\" section to create a new job from the new CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(base_dir, \"CSV\")\n",
    "results_job_id = 1468933 #job ID for the results we need\n",
    "next_stage = 'fgbg'\n",
    "job_id_to_copy = 1472963 # instructions to copy for the 'next_stage' job we're creating now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# identifier = create_next_CSV(csv_dir, results_job_id, next_stage)\n",
    "identifier = 'HeLa-S3_cyto_movies_s7'\n",
    "fig_eight(csv_dir, \"{0}_{1}\".format(identifier, next_stage), job_id_to_copy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
