{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing for Crowd Annotation Pipeline\n",
    "\n",
    "1. Download job report from Figure Eight\n",
    "2. Download annotated images from report\n",
    "3. Clean up montages (remove small objects, fill small holes, sequentially label the annotations)\n",
    "4. Process montages into movies\n",
    "5. Process annotations and raw images to match each other in size. Can either:\n",
    "    - chop raw images into corresponding pieces\n",
    "    - recombine annotation subimages into full-size images\n",
    "6. Combine raw and annotated movies into npz format\n",
    "7. Images are now ready to be used as training data or to track!\n",
    "\n",
    "Files are named by these scripts such that the code blocks can run back-to-back with minimal input. For this reason, it is recommended that users run through the whole pipeline before processing another set of images. The user can specify a few directory names and the \"identifier\" used in pre-annotation and run all cells in the notebook; alternate folder names can be used but this is not recommended.\n",
    "\n",
    "To function properly, your working folder should already contain subfolders:\n",
    "- json_logs  \n",
    "    - log from overlapping_chopper ({identifier}\\_overlapping_chopper\\_log.json)\n",
    "    - log from montage_maker ({identifier}\\_montage\\_maker.json)\n",
    "- raw images (can be named \"raw\" or something else)\n",
    "\n",
    "The user will also need to supply:\n",
    "- job ID for the data to download from figure eight\n",
    "- API key for figure eight\n",
    "- \"identifier\" to access correct json logs and name files correctly\n",
    "\n",
    "If the default folder names are used, by the end of this pipeline, the working folder (base_dir) will contain subfolders named:\n",
    "\n",
    "- CSV  \n",
    "    - data that was uploaded to figure eight in pre-annotation notebook  \n",
    "    - job report downloaded from figure eight\n",
    "\n",
    "- annotations  \n",
    "    - downloaded montages from figure eight, cleaned\n",
    "\n",
    "- movies  \n",
    "    - subfolders for different parts and subsections of full movie \n",
    "            - subfolders for holding raw and annotated data  \n",
    "                - images\n",
    "                \n",
    "- npz\n",
    "    - .npz where each montage has been turned into one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import statements\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "import stat\n",
    "import sys\n",
    "\n",
    "from deepcell_toolbox.utils.io_utils import get_img_names\n",
    "from deepcell_toolbox.pre_annotation.overlapping_chopper import overlapping_crop_dir\n",
    "\n",
    "from deepcell_toolbox.post_annotation.download_csv import download_and_unzip, save_annotations_from_csv\n",
    "from deepcell_toolbox.post_annotation.clean_montages import clean_montages, relabel_montages, convert_grayscale_all\n",
    "from deepcell_toolbox.post_annotation.montages_to_movies import all_montages_chopper\n",
    "from deepcell_toolbox.post_annotation.post_annotation_training_data import post_annotation_make_training_data\n",
    "\n",
    "perm_mod = stat.S_IRWXO | stat.S_IRWXU | stat.S_IRWXG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set working directory\n",
    "#base_dir = \"/base/directory/path/here\"\n",
    "#raw_dir = \"/base/directory/path/here/folder_with_fullsize_raw_images\"\n",
    "\n",
    "base_dir = \"/gnv_home/data/testing/post_processing/set3\"\n",
    "raw_dir = \"/gnv_home/data/testing/post_processing/set3/FITC_overlay_phase\"\n",
    "\n",
    "#identifier given during pre-annotation pipeline; if you're not sure, it's also in the job report csv\n",
    "identifier = \"set0_cyto_overlay\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Download job report from Figure Eight\n",
    "By default, this script will download, unzip, and rename the full report from Figure Eight as a .csv file. However, the user can change the report type if one of the other report options is more suitable for their use. (support for other report types not guaranteed with version 0 of this notebook)\n",
    "\n",
    "The user can specify where the zip file should be downloaded and the .csv extracted; by default, the .csv file will be put into a subfolder named CSV (likely the same folder that contained the input data; the CSV files are named to prevent confusion). The report CSV will be renamed \"job\\_{job_number}\\_{type of report}\\_report.csv\".\n",
    "\n",
    "#### From Figure Eight website:\n",
    "full - Returns the Full report containing every judgment\n",
    "\n",
    "aggregated - Returns the Aggregated report containing the aggregated response for each row\n",
    "\n",
    "json - Returns the JSON report containing the aggregated response, as well as the individual judgments\n",
    "\n",
    "gold_report - Returns the Test Question report\n",
    "\n",
    "workset - Returns the Contributor report\n",
    "\n",
    "source - Returns a CSV of the source data uploaded to the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_id_to_download = 1363594\n",
    "job_type = \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_unzip(job_id_to_download, base_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use report to download annotations\n",
    "This script uses the information in the report to download each annotation. Montage annotations will be saved in the \"annotations\" subfolder (it will be created for you by the script).\n",
    "\n",
    "Raw images that could not be annotated (those with \"broken_link = True\") will not be downloaded in this step. If a job contains rows with broken links, the information will be put into two csv files: one, \"job_number_full_report_broken_links.csv\", contains all of the metadata from the full job report, in case the user wants to inspect this for a pattern in the broken links. The other, \"job_number_reupload.csv\", has only the information used to upload the images originally (identifier and annotation_url). If the images are suitable for annotation, the user can easily add this csv to the figure eight job and obtain annotations for those images. (Alternatively, the user may need to go through part of the pre-annotation pipeline to adequately fix and reupload the images in question.)\n",
    "\n",
    "If there are no broken links in the job, or if images with broken links instead of annotations have annotations later on in the job report (if the user has reuploaded those rows to the job), the secondary csv creation will not be triggered.\n",
    "\n",
    "This function returns a list of the image names of any images with broken links; this list will be used later in the pipeline to automatically skip images when stitching images together or making training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = os.path.join(base_dir, \"CSV\")\n",
    "csv_path = os.path.join(csv_dir, \"job_\" + str(job_id_to_download) + \"_\" + job_type + \"_report.csv\")\n",
    "\n",
    "#csv_path = \"/example/path/CSV/job_number_full_report.csv\"\n",
    "\n",
    "montage_dir = os.path.join(base_dir, identifier + \"_montaged_annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_annotations_from_csv(csv_path, montage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Clean up the montages\n",
    "First, the RGB montage annotation is converted into grayscale, simplifying downstream use of the annotation.\n",
    "\n",
    "Next, small changes to the morphology of the image are made. Sometimes during annotation, small holes or stray annotations will be submitted, as artifacts of the annotation process. However, these holes or stray pixels don't correspond to what should be annotated, so in this step, we use sci-kit image to fix these small mistakes.\n",
    "\n",
    "Currently uses the old \"clean_montage\" function; this may change in future versions of notebook.\n",
    "\n",
    "After cleaning the montage, user can optionally run \"relabel_montages\" block, which will relabel the annotations sequentially (eg, perhaps the annotator decided to use the labels 3, 5, and 7 to label cells; this code block would remake the image with labels 1, 2, and 3).\n",
    "\n",
    "The cleaned and relabled annotations will overwrite the downloaded annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convert_grayscale_all(montage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_montages(montage_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "relabel_montages(montage_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process montages into movies\n",
    "\n",
    "Each montage is composed of frames of a timelapse (or sometimes, a z-stack) that have been placed next to each other. This is useful for annotators, but we want to use these images frame by frame in movies. This section of the notebook takes montages, as well as the parameters used to make the montage (such as spacing between frames) to chop one montage into its constituent frames. These sequential frames will then be saved in subfolders together.\n",
    "\n",
    "By default, these will be saved in a \"movies\" folder containing subfolders corresponding to the crop location of each montage (eg, x_1_y_0). Each subfolder will then contain a folder for the annotations of that position. The annotations folder will contain the image files for each frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json parameters\n",
    "json_montage_log_path = os.path.join(base_dir, \"json_logs\", identifier + \"_montage_maker_log.json\")\n",
    "try:\n",
    "    with open(json_montage_log_path) as json_file:\n",
    "        json_montage_log = json.load(json_file)\n",
    "except:\n",
    "    print(\"No montage maker log file found. Is the path to the json file correct?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_montages_chopper(base_dir,\n",
    "                     montage_dir, \n",
    "                     identifier, \n",
    "                     json_montage_log)\n",
    "\n",
    "chopped_annotations_dir = os.path.join(base_dir, 'chopped_annotations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Match raw images to annotations\n",
    "This section of the notebook will chop up raw images to match the size of the annotations extracted from montages. Then, files will be rearranged into a \"movies\" folder to match raw and annotated images in subfolders corresponding to the montage they came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chop the raw images into the same size pieces as the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read json parameters\n",
    "json_chopper_log_path = os.path.join(base_dir, \"json_logs\", identifier + \"_overlapping_chopper_log.json\")\n",
    "try:\n",
    "    with open(json_chopper_log_path) as json_file:\n",
    "        json_chopper_log = json.load(json_file)\n",
    "except:\n",
    "    print(\"No overlapping_chopper log file found. Is the path to the json file correct?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_x_segments = json_chopper_log['num_x_segments']\n",
    "num_y_segments = json_chopper_log['num_y_segments']\n",
    "overlap_perc = json_chopper_log['overlap_perc']\n",
    "try:\n",
    "    frame_offset = json_chopper_log['frame_offset']\n",
    "except:\n",
    "    frame_offset = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlapping_crop_dir(raw_dir, \n",
    "                     identifier + \"_raw\", \n",
    "                     num_x_segments, \n",
    "                     num_y_segments, \n",
    "                     overlap_perc, \n",
    "                     frame_offset, \n",
    "                     is_2D = False)\n",
    "\n",
    "raw_pieces_dir = raw_dir + \"_offset_{0:03d}_chopped_{1:02d}_{2:02d}\".format(frame_offset, num_x_segments, num_y_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rearrange the image files into a \"movies\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_dir = os.path.join(base_dir, \"movies\")\n",
    "if not os.path.isdir(movies_dir):\n",
    "    os.makedirs(movies_dir)\n",
    "    os.chmod(movies_dir, perm_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_img_list = get_img_names(raw_pieces_dir)\n",
    "annotated_img_list = get_img_names(chopped_annotations_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = json_montage_log['montages_in_pos']\n",
    "montage_len = json_montage_log['montage_len']\n",
    "\n",
    "for part in range(parts):\n",
    "    start_frame = part * montage_len\n",
    "    \n",
    "    for i in range(num_x_segments):\n",
    "        for j in range(num_x_segments):\n",
    "            subfolder = 'x_{0:02d}_y_{1:02d}_part_{2}'.format(i,j,part)\n",
    "            subdir = os.path.join(movies_dir, subfolder)\n",
    "            if not os.path.isdir(subdir):\n",
    "                os.makedirs(subdir)\n",
    "                os.chmod(subdir, perm_mod)\n",
    "            \n",
    "            annotation_subdir = os.path.join(subdir, \"annotated\")\n",
    "            if not os.path.isdir(annotation_subdir):\n",
    "                os.makedirs(annotation_subdir)\n",
    "                os.chmod(annotation_subdir, perm_mod)\n",
    "            \n",
    "            raw_subdir = os.path.join(subdir, \"raw\")\n",
    "            if not os.path.isdir(raw_subdir):\n",
    "                os.makedirs(raw_subdir)\n",
    "                os.chmod(raw_subdir, perm_mod)\n",
    "            \n",
    "            #move raw images into subfolders\n",
    "            for frame in range(montage_len):\n",
    "                raw_name = 'x_{0:02d}_y_{1:02d}_frame_{2:03d}.'.format(i,j, start_frame + frame)\n",
    "                for raw_img in raw_img_list:\n",
    "                    if raw_name in raw_img:\n",
    "                        shutil.copy(os.path.join(raw_pieces_dir, raw_img), raw_subdir)\n",
    "            \n",
    "            #move annotations into subfolders\n",
    "            annotation_name = subfolder + \"_frame\"\n",
    "            for annotated_img in annotated_img_list:\n",
    "                if annotation_name in annotated_img:\n",
    "                    shutil.copy(os.path.join(chopped_annotations_dir, annotated_img), annotation_subdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine raw and annotation images into .npz formatted training data\n",
    "Currently, this section allows the user to go through part-level directories in a \"movies\" folder and combine the images contained therein to make training data. The training data is saved as an .npz file in the \"npz\" folder that will be created inside the \"base_dir\" (specified at beginning of notebook). (This default setting can be changed by specifying a different \"output_dir\" below.)\n",
    "\n",
    "Feature to be added: include \"full movie\" mode that recombines all parts; this would allow movies longer than the montage length to be tracked. However, the resulting .npz file wouldn't be suitable for use as tracking training data (at least, without curation first) because track continuity is not maintained between parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_names = ['raw']\n",
    "annotation_folders = ['annotated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join(base_dir, \"npz\")\n",
    "\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    os.chmod(output_dir, perm_mod)\n",
    "\n",
    "#Training directories are organized according to location within an image\n",
    "#if there are any movies that shouldn't be included in the npz\n",
    "#(unsuitable for training, or don't need to be tracked), put them in \"samples_to_drop\"\n",
    "samples_to_drop = []\n",
    "training_folders = ['x_{0:02d}_y_{1:02d}_part_{2}'.format(i,j,part) for part in range(parts) for i in range(num_x_segments) for j in range(num_y_segments)]\n",
    "training_folders = [x for x in training_folders if x not in samples_to_drop]\n",
    "\n",
    "npz_name = \"{0}_montaged.npz\".format(identifier)\n",
    "file_name_save = os.path.join(output_dir, npz_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "post_annotation_make_training_data(movies_dir,\n",
    "                                   file_name_save,\n",
    "                                   channel_names,\n",
    "                                   annotation_folders,\n",
    "                                   reshape_size = None,\n",
    "                                   dimensionality = 3,\n",
    "                                   num_frames = montage_len,\n",
    "                                   training_folders = training_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the result\n",
    "data = np.load(file_name_save)\n",
    "X_to_load, y_to_load = data['X'][()], data['y'][()]\n",
    "\n",
    "print(data.keys())\n",
    "data_readable_X, data_readable_y = data['X'][()], data['y'][()]\n",
    "print('X Shape:', data_readable_X.shape)\n",
    "print('y Shape:', data_readable_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
